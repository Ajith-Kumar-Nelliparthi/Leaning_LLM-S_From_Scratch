![alt text](image.png)
# Pause Tokens, Infinite Context Window, Bidirectional LLMs
- read the paper [Think before you speak: Training Language Models With Pause Tokens](https://arxiv.org/abs/2310.02226)
- read the paper [Meet in the Middle: A New Pre-training Paradigm](https://arxiv.org/abs/2303.07295)
- watched ðŸŽ¥: [Google just Solved the Context Window Challenge for Language Models](https://youtu.be/ANjEFi2lkXQ?si=FwCzyGn1w1qoQavB)
- watched ðŸŽ¥: [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](https://youtu.be/r_UBBfTPcF0?si=hf9jMEdJwlbTDVWL)
- read the paper [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](https://arxiv.org/abs/2404.07143)