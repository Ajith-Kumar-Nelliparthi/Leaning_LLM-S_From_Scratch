![alt text](image.png)
# Rotary Positional Embedding
- ## ROPE
 - ðŸŽ¥ [RoPE (Rotary positional embeddings) explained: The positional workhorse of modern LLMs](https://youtu.be/GQPOtyITy54?si=P9ka7cLWwVjgghc9)
 - ðŸŽ¥ [Rotary Positional Embeddings: Combining Absolute and Relative](https://youtu.be/o29P0Kpobz0?si=X1ulnN6Vd4KeegM7)

 - read the original paper: [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)

 - ðŸŽ¥ watched [The KV Cache: Memory Usage in Transformers](https://youtu.be/80bIUggRJf4?si=TdAZ4nFAzHOnB-lL)