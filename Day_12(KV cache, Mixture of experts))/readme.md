![alt text](image.png)

# KV Cache, Mixture of Experts
> ### MOE
  > watched : [Mistral 8x7B Part 1- So What is a Mixture of Experts Model?](https://youtu.be/RYZ0FMAKRFs?si=8IoUX5BA_cFZGbfp)
  - this is wrap for llm topics , next step is to recap all what we learned. reviewing llama architecture is the best way to do this.

> ### Llama
 - ðŸŽ¥ [LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU](https://youtu.be/Mn_9W1nCFLo?si=MPjvnfXYiIElyQbr)
 - ðŸŽ¥ [Let's build GPT: from scratch, in code, spelled out.](https://youtu.be/kCc8FmEb1nY?si=SmEu8igJLrbklhKW)